---
layout: "page"
title: Second Order Methods
order: 1.3
---

_"Accelerate when the road is straight, slow down when it is curved" -- Yuli_

<div>
    <body>
        <iframe src="{{ site.baseurl }}/assets/second_derivative.html" width="900" height="570"></iframe>
    </body>
</div>

Another very useful information that guides the search of stepsize is the second order derivative (generalized to Hessian matrix in the multi-dimensional setting) at the current location of each step. 

**Newtonâ€™s method**: calculate the 2nd order derivative (Hessian) at each step, and use its inverse as step size.



Quasi-Newton methods: calculating Hessian is expensive, how about we approximate it?

[**GaussNetwon**](/algorithms/GaussNewton.html): use the vector square of the 1st order derivative (Jacobian) to approximate Hessian, and inverse the approximated Hessian as a step size. 

**Levenberg Marquardt**: similar to GaussNetwon, but add a regularization term (a multiple of Identity matrix) to the Hessian approximation at each step to stabilize the optimization process. 

[**DFP**](/algorithms/DFP.html): Starts the approximation of Hessian Inverse with Identity matrix, and updates it using step and gradient info in each step. 

[**BFGS**](/algorithms/BFGS.html): Similar as DFP, with additional capability to preserves the positive symmetry of Hessian matrix.

### References:

https://www.youtube.com/watch?v=Kln0ZQ7sX8k&list=LL&index=38